{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reiniscimurs/gnn_with_pytorch/blob/main/section_4/02_gcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBeK8bWc4a_9"
      },
      "source": [
        "# Implementation of GCN\n",
        "We will implement Graph Convolutional Networks (GCN). Utilizing a dataset with multiple graphs as training data, learning will be performed through mini-batch training.\n",
        "As GPU will be used for training, let's select \"GPU\" under \"Hardware Accelerator\" in \"Runtime\" -> \"Change runtime type.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztIkOM_N5Bu7"
      },
      "source": [
        "## Installation of PyTorch Geometric\n",
        "Install the library \"PyTorch Geometric\" for Graph Neural Networks (GNN), as well as related libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLrzxL39qGie"
      },
      "outputs": [],
      "source": [
        "!pip install pyg-lib torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu116.html\n",
        "!pip install torch-geometric\n",
        "!pip install scipy==1.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZeLe5ktmDCG"
      },
      "source": [
        "## Loading the Dataset\n",
        "Load the dataset \"MUTAG\" from TUDataset, which contains 188 graphs.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlyKILeKUskH"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "dataset = TUDataset(root=\"/tmp/MUTAG\", name=\"MUTAG\")\n",
        "\n",
        "dataset = dataset.shuffle()  # Shuffle the dataset\n",
        "dataset_train = dataset[:140]  # Training dataset\n",
        "dataset_test = dataset[140:]  # Test dataset\n",
        "\n",
        "batch_size = 64  # Batch size\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2s4JTtLh_cO"
      },
      "source": [
        "## Model Construction\n",
        "We will build the model for GCN.\n",
        "For the layer implementation, we will use GCNConv() and configure it as follows:\n",
        "```\n",
        "GCNConv(input_feature_size, output_feature_size)\n",
        "```  \n",
        "https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv  \n",
        "This time, we will introduce \"dropout,\" which randomly removes neurons before the fully connected layer.\n",
        "By introducing dropout, we can train the model to be more robust to unknown data.  \n",
        "https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGMeG-bgldbu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "n_h = 64  # Number of features in the hidden layer\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(dataset.num_node_features, n_h)\n",
        "        self.conv2 = GCNConv(n_h, n_h)\n",
        "        self.conv3 = GCNConv(n_h, n_h)\n",
        "        self.fc = nn.Linear(n_h, dataset.num_classes)  # Fully connected layer\n",
        "\n",
        "        self.relu = nn.ReLU()  # ReLU\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout: (p=dropout rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x = data.x\n",
        "        edge_index = data.edge_index\n",
        "        batch = data.batch\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Take the mean of each feature for all nodes\n",
        "        x = global_mean_pool(x, batch)  # Convert to (batch size, number of features)\n",
        "\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "net = GCN()\n",
        "net.cuda()  # Enable GPU support\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`global_mean_pool()` takes the mean of each feature along the \"node dimension,\" transforming the data into the shape (batch size, number of features)."
      ],
      "metadata": {
        "id": "R7L6NM_Zin21"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP9O1HhFikx-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare a function for model evaluation."
      ],
      "metadata": {
        "id": "RvQn3Apg_Ig_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(loader):\n",
        "    correct = 0  # Number of correct predictions\n",
        "\n",
        "    for data in loader:\n",
        "        data = data.cuda()  # GPU support\n",
        "        out = net(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += int((pred == data.y).sum())\n",
        "\n",
        "    return correct / len(loader.dataset)  # Accuracy"
      ],
      "metadata": {
        "id": "vwqxhW52_Phq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model using the training data."
      ],
      "metadata": {
        "id": "cLmtIpUR_Olw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfv35TJsnNcZ"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "# Cross-entropy loss function\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimization algorithm\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "for epoch in range(200):\n",
        "    # Training\n",
        "    net.train()  # Training mode\n",
        "    for data in loader_train:\n",
        "        data = data.cuda()  # GPU support\n",
        "\n",
        "        optimizer.zero_grad()  # Step 1: Initialize gradients\n",
        "        out = net(data)  # Step 2: Forward pass to obtain predictions\n",
        "        loss = loss_fnc(out, data.y)  # Step 3: Compute loss from predictions and ground truth\n",
        "\n",
        "        loss.backward()  # Step 4: Backpropagation to compute gradients\n",
        "        optimizer.step()  # Step 5: Update parameters using the optimization algorithm\n",
        "\n",
        "    # Evaluation\n",
        "    net.eval()  # Evaluation mode\n",
        "    acc_train = eval(loader_train)\n",
        "    acc_test = eval(loader_test)\n",
        "    print(\"Epoch:\", epoch,\n",
        "          \"acc_train:\", str(acc_train * 100) + \"%\",\n",
        "          \"acc_test:\", str(acc_test * 100) + \"%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3rn17KTjLSr"
      },
      "source": [
        "## Model Evaluation\n",
        "Evaluate the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7K0lvg0nQL4"
      },
      "outputs": [],
      "source": [
        "net.eval()  # Evaluation mode\n",
        "acc_test = eval(loader_test)\n",
        "print(\"Accuracy:\", str(acc_test * 100) + \"%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}